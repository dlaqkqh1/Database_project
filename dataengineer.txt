* 오랜 경력의 데이터 엔지니어가 쓴 글이 아님을 밝힌다. 최근 회사에서 면접관으로 면접을 보고 있는데, 회사마다 데이터 관련 포지션은 하는 일이 다르기 때문에 참고만 하는 것이 좋겠다. 오랫동안 데이터 관련 포지션으로 일하기를 희망했는데, 관련된 정보를 찾기 힘들어서 비슷한 길을 걷기를 희망하는 개발자 분들을 위해서 정보 공유 차원에서 남긴다.* 주제의 특성상 개발에 대한 이해가 있는 독자를 대상으로 글이 쓰였음을 미리 밝힌다. 데이터 엔지니어로 일한 지 3개월이 되었다. 이전에는 백엔드 건, 프런트엔드 건, 모바일 앱 개발이건 모두 서비스 자체를 만드는 일을 주로 해왔다. 이번에는 그 서비스 뒤에서 생성되는 방대한 데이터를 어떻게 가공하고 활용할지 생각하는 작업을 하고 있다. 함께 일하는 사람들이 관심을 갖는 분야가 전혀 다르다 보니 제법 낯선 느낌인데, 그래도 많은 것을 배울 수 있는 기회라 굉장히 즐겁다. 데이터 중심의 회사임에도 불구하고 그 데이터를 실제 클라이언트에게 가공해서 보여주는 일까지 하는 회사 비즈니스 모델의 특성상, 데이터 생성 시점부터 시작해서, 데이터 가공, 데이터 시각화까지 다양한 부분을 다뤄볼 수 있다는 것이 매우 매력적인 회사다. 회사에 대해서 간단히 소개하자면 실리콘밸리에 본사가 있는 미국계 회사인데, B2B로 무선 네트워크를 제공하고 있다. 예를 들면, 지하철이나 건물 하나 통째로 무선 인터넷을 공급하는 역할을 하고 있다. 이 과정에서 하드웨어와 소프트웨어를 둘 다 제공하는데, 싱가포르의 데이터 엔지니어링 팀은 하드웨어에서 발생하는 데이터를 가공해서 클라이언트에게 전달하는 일을 하고 있다. 경쟁자로는 CISCO가 대표적이고, 굳이 따지자면 하는 포지션은 IoT 기반의 데이터 엔지니어라고 봐도 무방할 것 같다.   데이터 엔지니어로 일하기 전 이 분야에서 일을 하기 전에는 데이터 분석이라는 업무에 대한 막연한 동경을 가지고 있었음을 고백한다. 원래부터 비즈니스에 관심이 많아서, 한국에서 스타트업을 다닐 때도 혼자서 그로스 해커(Growth Hacker)라는 직함을 달고 누가 시키지 않아도 데이터를 혼자서 가공하고 인사이트를 찾아내는 것을 즐기기도 했다. 그래서 데이터 엔지니어도 비슷한 일을 하지 않을까 생각했다. 그래서 데이터 관련 업무를 하기 위해서, 파이썬으로 만들어진 데이터 분석 라이브러리인 Numpy나 Pandas를 공부하기도 했고, Jupyter Notebook을 통해서 데이터 시각화해보기도 하고, Tensorflow 같은 기술들을 바탕으로 머신러닝과 딥러닝에 대해서 이해하기 위해서 노력하기도 했다. 데이터가 이쁘게 있으면 그걸 분석할 생각만 했지, 그 데이터가 어디에서 나오는지에 대해서는 깊이 있게 생각해보지 않았다고 보는 것이 맞겠다. 그런데 지금은 그 분석을 하기 위해 나오는 데이터들이 쉽게 얻어지지 않는다는 사실을 깨달았다. 데이터는 데이터베이스에 곱게 저장되어있는 경우도 많지만, 많은 경우에는 파일의 형태로 로그(log) 파일에 저장되어 있는 경우도 많다. 물론 데이터베이스에 있는 데이터라고 다 바로 활용할 수 있는 건 아니고, 많은 경우에 데이터를 깔끔하게 만드는 작업을 해야 한다. 혹은 데이터의 속도가 중요한 경우라면 스트리밍을 통해서 실시간으로 데이터를 처리하는 경우도 많다. 스트리밍은 동영상에만 하는 것인 줄 알았는데, 데이터도 스트리밍으로 가공을 해야 하는 것일 줄이야. 그리고 이렇게 뽑아낸 자료에서 최종 결과물을 뽑아내기 위해서 데이터를 가공하는 작업을 하는데, 보통 이 작업을 Extract, Transform, Load를 줄여서 ETL이라고 부른다. 이렇게 가공된 데이터를 머신러닝이나 딥러닝 모델에 집어넣거나, CEP(complex event processing)이라고 불리는 다양한 이벤트 간의 연관성에서 인사이트를 찾는 방식으로 데이터를 처리하기도 한다. 문제는 이 일련의 과정이 굉장히 방대한 데이터를 처리해야 하고, 또한 거의 실시간에 가깝게 처리되기 때문에 서비스의 안정성이 매우 종요하다는 점이다. 특히 데이터에서 뽑아낸 인사이트는 시간이 지나면 효용이 떨어지는 경우도 많기 때문에 최대한 빠르게 결과물을 얻기 위해서 정말 많은 고민을 하게 된다. 지금 있는 회사에는 데이터 엔지니어들과 함께 데이터 사이언티스트들이 별도로 존재하는데, 데이터 사이언티스트는 주로 데이터 모델링에 더 초점을 맞춰서 일을 한다. 그런데 이렇게 만들어진 모델이나 알고리즘이 실제로 돌아가는 인프라에서는 제대로 작동하지 않은 경우가 많기 때문에 그 작동 원리를 이해하고 최대한 효과적으로 데이터를 처리할 수 있도록 변경 및 개선 작업을 하는 것이 데이터 엔지니어들이 주로 하는 일이다. 나는 어떻게 데이터 엔지니어가 되었나? 나는 앞서 이야기했던 것처럼 데이터 분석 관련 업무에 관심이 많았다. 비즈니스에 관심이 많기 때문에 자연스럽게 데이터에 관심이 많았고, 그래서 이런저런 데이터 분석 관련 기술에 대해서 공부했던 것 같다. 하지만 결정적으로는 내가 사용하는 기술들이 도움이 되었다. 나는 지난 회사에서 스칼라(Scala)라는 프로그래밍 언어를 사용하게 되었고, 그리고 기존에 파이썬 개발을 많이 했었는데, 이 두 가지 언어가 데이터 분야에서 가장 많이 활용되는 기술이었다. 그리고 빅데이터 분야에서 많이 활용되는 카프카(Kafka)나 카산드라(Cassandra)를 활용해본 것도 큰 도움이 되었다. 물론 이런 것들을 알고 있었다고 해도 이 회사에 들어와서 배워야 하는 것들은 정말 끝도 없이 많았다. 기본적으로 하둡(Hadoop)과 스파크(Spark) 기반으로 일을 하고 있고, IoT 기반이다 보니 방대한 데이터를 처리하기 위해서 MQTT에 대해서 이해하는 것도 필요했다. 그리고 기존에는 다양한 MySQL 같은 관계형 데이터와 MongoDB, Cassandra 등 NoSQL 데이터베이스 등만 제한적으로 활용했다면, 빅데이터의 세상에 오니 Druid, HBase 같은 정말 듣지도 보지도 못한 온갖 종류의 데이터베이스의 세상이 펼쳐졌다. 그리고 이것을 단순히 사용하는 것에서 그치는 것이 아니라 성능을 개선하기 위해서 끊임없이 튜닝하고 모니터링하는 작업도 필수적이었다. 그리고 클라우드 기반에서 일을 하다 보니 개발 환경을 잡는 작업에 대해서 얼마만큼의 비용이 발생할지 산정하는 것도 중요한 일이었고, 빅데이터 분야 관련해서 정말 많은 3rd Party 라이브러리들이 많다 보니 그걸 어떻게 학습하고 다른 사람들과 공유할까 고민하는 것도 중요 업무 중에 하나였다.  아래 그래프처럼 나의 경력의 대부분은 백엔드와 프런트 개발 쪽에 맞춰져 있는데, 이번 회사에 입사하면서 데이터 엔지니어링 분야에도 전문성을 쌓기 시작했다. 앞으로는 조금 더 데이터와 그 바탕이 되는 시스템 운영까지 관심사를 넓혀보려고 생각하고 있다.    일해보니,특정 산업에 대한 지식을 도메인 지식이라고 부른다. ETL을 포함한 데이터 엔지니어링 프로세스 자체는 도메인 지식과는 독립적인 경우가 많지만, 데이터 엔지니어링을 제대로 하기 위해서는 도메인 지식이 상당히 요구된다. 그래서 그런지 회사에 7~20년 차 경력의 개발자들이 즐비하고, 이 회사에만 5년을 넘게 다닌 사람도 제법 된다. 거의 실시간으로 방대한 데이터가 처리되는 거대한 파이프라인 속에서 일하기 때문에, 개발자들이 시스템의 안정성과 확장성에 대한 어마어마한 집착이 있는 경우가 많다. 일단 만들고, 서비스가 몇 번 죽고 나면 그 이후에 서비스 안정성을 고민하는 스타트업과는 전혀 다른 분위기에 처음에는 어리둥절 했는데, 그 고민하는 과정을 옆에서 지켜보며 많은 것을 배우고 있다. 이런 환경이다 보니 개발자들 절반 이상이 Docker나 Kubernetes를 다루고, shell script를 직접 만들고, AWS에 인스턴스를 띄워서 개발 환경을 구축할 수 있는 DevOps에 준하는 시스템 관리 지식과 경험을 갖고 있는 경우가 많은 거 같다. 이런 지식이 없는 상태에서 회사에 들어왔다고 하더라도, 거의 일상적으로 이뤄지는 일이 이런 분야다 보니 빠르게 습득할 것을 요구받기도 한다.데이터 엔지니어로 일을 시작하고 나서, 데이터 관련 일을 하고 싶어서 조언을 구한다는 이메일을 종종 받는다. 그런데 왜 데이터 사이언티스트나 데이터 엔지니어가 되고 싶냐고 물어보면 "개발은 어려워 보이는데, 데이터 관련 업무는 비교적 쉽게 할 수 있을 거 같아서"라는 답변을 듣는 경우가 많다. 그런데 함께 일하는 데이터 사이언티스트들도 직접 인프라 관련 코드를 짜지는 않더라도, 데이터베이스나 시스템 관련 지식에 대해서는 해박한 경우가 많고, 파이썬이나 자바, 스칼라 등 최소한 한 언어로 원하는 알고리즘을 구현할 수 있을 만큼 프로그래밍에 익숙하다. 그래서 최근에는 그런 질문을 받으면 데이터 관련 석사를 하거나 백엔드 분야로 경력을 먼저 쌓으시라고 조언해드리고 있다. 데이터 분석을 위해서 필요한 것은 그 데이터가 어떻게 만들어지고, 어떻게 시스템 상에서 구현되어야 하는지 전체를 이해하는 일이 아닌가 싶다.

안녕하세요. 간략하게 제 소개를 드리면, 현재 데이터 엔지니어로 일 하고 있고 해당 분야에 관련된 질문이 잇다에서 들어와 다른 분들도 보실 수 있게끔 에세이로 남기면 좋겠다 싶어 글을 쓰게 되었습니다. ^^

데이터쪽도 정말 많은 분야가 있어서 이 글 에서 전부 깊게 다루기는 어려울 것 같고 제가 알고 있는 내용 / 온라인에 올라온 현직자의 경험담 / 실제 회사의 채용 공고 등을 통해서 같이 알아보면 좋을 것 같습니다.

우선 저는 Data Infrastructure 라는 팀에서 근무 하고 있습니다. 팀의 역할은 다음과 같습니다.

* 데이터를 수집 / 정제하고
* 비즈니스적으로 가치 있는 데이터를 직접 발견하거나 
* 다른 구성원들이 데이터를 쉽게 활용할 수 있도록 돕습니다
* 또한 데이터를 눈으로 보고 인사이트를 얻는것에서 그치는게 아니라, 실제 서비스까지 제공합니다 (실시간 인기 검색어 같은) 
* 이 과정에서 필요한 툴과 정책 등을 다른 사람들에게 제공하고, 필요하면 직접 만들기도 합니다.

- 데이터 엔지니어링
주요 수행업무
- 데이터 수집, 가공, 적재 처리(ETL 개발)
- DW 구축, 데이터 모델 설계 및 개발
- Cloud 기반 빅데이터 분석 환경 구축

경력 요건
자격요건
- DBMS 사용 경험, SQL 작성 능력 (필수)
- Python, Java, shell script 등 프로그래밍/스크립트 언어 이용한 개발 경험
우대사항
- DMP 구축 및 유관업무 경험자 우대
- Cloud(AWS, GCP 등) 환경에서 개발 및 운영 경험자 우대

# 데이터 파이프라인

위 내용 중, 데이터를 수집하고, 정제하고 다시 내보내는 과정이 포함되어 있고 이 것을 보통 데이터가 흘러다니는 길이라 해서 데이터 파이프라인이라 부릅니다. 

조금 더 부연 설명을 해 보자면,

1. 정의 Schema

데이터는 그냥 생성되지 않습니다. 어떤 형태 (Schema) 로 어떤 값들이 언제 남아야하는지 누군가는 정해야 하고, 이 정해진 규칙에 따라 남겨야 합니다. 

예를 들어 Client (앱, 브라우저) 로그라면 사용자의 활동을 구분하기 위해 session 등의 값과 차후 푸시 마케팅 등을 위해 adid 등을 포함할 수 있습니다. 

2. 로깅 Logging

단순히 모양을 정한다고 해서 데이터가 남지는 않습니다. 데이터를 남기는 활동을 로깅 (Logging) 이라 부르는데, 사용자의 활동을 앱에서 GA (Google Analytics) 처럼 바로 보낼 수도 있고 아니면 서버에서 이벤트를 Database 에 저장하고 그것을 옮겨 가거나 아니면 API 의 로그를 파일로 남겨 긁어가는 방법도 있습니다. 각각의 방법 모두 활용처와 장단점이 있기 때문에 보통 섞어서 사용합니다.  

3. 수집 Collection

데이터를 어디론가 전송하거나, File 로 남겼거나 아니면 Database 에 저장했다면 이를 중앙화된 저장소에 옮깁니다. 이 과정에서 차후에 사용될 연산의 형태를 고려해 File Format 이나 (Parquet 등) 데이터의 값이 변경 (개인정보 암호화 / 마스킹 등) 될 수 있습니다.  또한 데이터가 조회될 형태, 빈도 그리고 데이터의 크기 등을 고려해 필요한 저장소를 결정합니다. (HDFS, Kafka, ...) 

예를 들어 
- Client 로그의 경우 서버로 바로 보낼 수 있고 (AWS 를 이용한다면 Kinesis 로 보내거나 등)
- Nginx Access 로그라면 파일로 남겨 같은 서버에서 동작하는 Agent (fleutnd 등) 이 다른 곳으로 전송할 수 있습니다
- Server 의 로그 (INFO, ERROR 등) 도 마찬가지로 Agent 에서 긁어 ElasticSearch 등의 저장소로 보낼수도 있을 테고
- 일별 회원이나, 거래 기록의 경우 Database 에 남아 있으므로 중요도에 따라 시간별 / 일별로 테이블 전체 Dump 를 뜰 수도 있겠습니다

4. 정제 Cleansing

로그는 발생 또는 전송 (수집 등 파이프라인 내 과정도 포함) 중에 유실이나 중복이 생길 수 있습니다. 또는 앱이나 서버의 버그로 인해 잘못된 데이터가 쌓이는 경우도 종종 있습니다.  따라서 데이터의 최종 사용자에게 전달하기 전 까지 정제 과정을 거칠 수 있습니다.

5. 조회 Retrieval / 집계 Aggregation

쌓인 데이터를 활용하는 형태는 크게 두 가지로 나눌 수 있습니다. 

* 데이터를 분석하기 위해 잦은 빈도로 데이터를 탐색하는 형태: 조건을 변경해 가며 데이터를 이리 저리 확인
* 정해진 데이터를 일정 주기마다 집계해 새로운 데이터를 생성: 실시간 / 시간별 / 일별로 테이블 생성 (일별 인기 검색어 등)

이 과정에서 각각의 사용 형태마다 사용되는 툴이 다릅니다. 집계를 실시간으로 해야 하는지, 일별로 집계해도 되는지, 데이터 사이즈가 엄청 큰지에 따라 도구가 달라질 수 있습니다. 또한 데이터를 소비해서 새로운 데이터를 만들어 내기 때문에, 데이터의 목적지 저장소에 따라서 또 다른 툴을 사용해야할 수 있습니다. (Spark, Presto, Kafka, Flink, EMR, ...) 

분석 하는 사람에 따라서도 도구가 달라질 수 있습니다. 

- 자신이 만든 기능이 많이 사용되는지 알고 싶은 디자이너나 기획자
- 통계 (사용자 수 등) 를 확인해야 하는 분석가
- 데이터를 이용해 서비스를 만들어 내는 (예를 들어 검색 광고라면, 검색어와 관련된 광고를 연결하고 불필요한 광고는 필터링 하는 등)  다른 팀의 다른 엔지니어가 있을 수 있습니다. 

따라서 데이터 엔지니어는 소비자 / 운영 비용 / 도구의 편리성 등을 모두 고려해 필요로 하는 툴들을 제공해야 합니다.
 
혹시나 데이터 파이프라인 / 데이터 엔지니어링 관련해서 더 궁금하시다면, Udemy 에서 데이터 엔지니어로 근무하시는 강대명님의 아래 슬라이드를 참고하시면 조금 더 자세한 내용을 확인하실 수 있습니다. ^^

* (Data Engineering 101) https://www.slideshare.net/charsyam2/data-engineering-101
* (Data Pipeline, Data Lake) https://www.slideshare.net/charsyam2/data-pipeline-and-data-lake


# 데이터 프로덕트

데이터를 보고 인사이트를 얻거나, 의사 결정에 활용하기도 하지만 이것 외에도 데이터를 이용해 서비스를 제공하기도 합니다. 보통 데이터 서비스, 데이터 프로덕트라고 부릅니다. 우리가 흔히 말하는 광고 / 추천 / 검색 / 통계가 모두 데이터 프로덕트로 볼 수 있습니다.

- 사용자의 브라우저 히스토리를 통해 다른 사이트에서도 실시간으로 보여지는 동일 상품의 광고
- 사용자가 본 결과에 따라 달라지는 추천 상품
- '야놀자' 에 대한 연관 검색어
- 시간별 / 일별 인기 여행지
- 해외 결제나 구매 패턴으로 비 정상적인 금융 거래 탐지
- 소셜 네트워크에서 새로운 친구 / 페이지 등 추천

이런 데이터 프로덕트는 사실 데이터 파이프라인의 가장 마지막 부분에 위치합니다. 데이터를 정의하고, 남기고, 쌓고, 정제한 후에 데이터를 조회해서 쓸모가 있을지 판단 한 후 비즈니스에 중요한 가치가 있다고 판단되면 데이터를 만들고 이것을 서비스에 지속적으로 제공합니다. 

다시 말해서 만들기 까지 시간은 가장 오래 걸리지만 비즈니스 가치를 만들어 내기 때문에 실제로는 가장 의미 있는 부분입니다. 데이터는 단순히 쌓고, 본다고 해서 가치를 가지는게 아니라 다시 돌아 서비스로 제공되야만 의미가 있습니다. 

조금 더 개인적인 견해를 덧붙이면, 서비스로 어떤 데이터가 제공된다는 말은 데이터가 비즈니스적으로 가치가 있어야 한다는 말이고 이는 저장하고 정제하고 분석하고 다시 사용하는 모든 활동에서 잊지 말아야 할 것이 "비즈니스 가치" 임을 말합니다. 결국 (데이터) 엔지니어 또한 비즈니스적인 가치를 만들어 내는 사람이므로 멋있고 최신의 기술을 쫓는데 많은 노력을 쏟기 보다는 비즈니스적으로 의미 있는 제품을 시간 내에 만들어 마켓에 제공하는데 목적을 두어야 합니다.


# 머신러닝

머신 러닝 관련해서는, 제가 잘 알지는 못하지만 지역 중고 직거래 마켓 앱인 당근마켓의 사례로 같이 알아 볼 수 있을 것 같습니다.

* (당근마켓에서 딥러닝 활용하기) https://medium.com/daangn/%EB%8B%B9%EA%B7%BC%EB%A7%88%EC%BC%93%EC%97%90%EC%84%9C-%EB%94%A5%EB%9F%AC%EB%8B%9D-%ED%99%9C%EC%9A%A9%ED%95%98%EA%B8%B0-3b48844eba62

위 글에 따르면, 당근 마켓에서는 중고 거래 게시글이 금지 품목인지 아닌지 판별하기 위해, 딥러닝을 활용한다고 나와있네요. 

문제는 서비스가 성장하면서 저희 소규모 인원만으로는 감당할 수 없을 정도로 신고량이 늘어나게 되었습니다. 그렇다고 스타트업에서 운영인력을 선형적으로 채용하기 부담이 될 수 밖에 없었습니다.

아래 글은 최근 올라온 글인데, 당근 마켓 인턴 분께서 이미지만으로 중고 물품의 카테고리를 분류하는 이야기에 관해서 소개해 주셨습니다.

* (이미지만으로 내 중고물품의 카테고리를 자동으로 분류해준다면?) https://medium.com/daangn/%EC%9D%B4%EB%AF%B8%EC%A7%80%EB%A7%8C%EC%9C%BC%EB%A1%9C-%EB%82%B4-%EC%A4%91%EA%B3%A0%EB%AC%BC%ED%92%88%EC%9D%98-%EC%B9%B4%ED%85%8C%EA%B3%A0%EB%A6%AC%EB%A5%BC-%EB%B6%84%EB%A5%98%ED%95%B4%EC%A4%80%EB%A9%B4-feat-keras-b86e5f286c71


# 검색 엔진

검색 관련해서도 제 분야는 아니지만, 아래 나온 블로그에 올라온 글을 통해서 어떤 지식이 필요한지 알아 볼 수 있을것 같습니다.

* https://ratsgo.github.io/blog/categories/

위 블로그 글 중에서 몇 개를 골라봤는데, 제목을 보면 한국어에 대한 도메인 지식과 통계 또는 수학적인 내용이 필요한 것 같습니다. 
더 궁금하신 내용이 있다면 위 블로그에서 한번 살펴보시면 좋을것 같습니다.

* CNN으로 문장 분류하기 19 Mar 2017
* 그래프로 중요 기사 걸러내기 13 Mar 2017
* Sequence-to-Sequence 모델로 뉴스 제목 추출하기 
* 통계 기반 감성사전 구축 25 Jun 2017
* 형태소 분석기 성능 비교
 주요업무
•﻿ 서비스 제공을 위한 다양한 금융 데이터 수집 백엔드/인프라 개발 및 운영
•﻿ 성능 최적화, 운영 자동화

자격요건
•﻿ Python 백엔드 개발 경력 3년차 이상
•﻿ AWS 환경에서 개발 및 운영 경험
•﻿ 데이터 소스로부터 데이터 파이프 라인과 플랫폼 구축 경험

우대사항
• AWS기반 데이터 파이프라인 구축에 대해 깊게 생각해보거나 경험이 있는 분
• ﻿ES를 구축하고 사용을 도와본 경험이 있어 자세하게 설명할 수 있는 분
• ﻿새로운 기술에 대한 관심이 많고 실제 서비스에 적용하여 개선한 경험
•﻿ 데이터 분석가와 커뮤니케이션이 가능한 분

- 통합데이터의 활용성과 편의성 증대를 위한 데이터 모델 설계 및 기획
- 모빌리티 데이터 엔지니어링 : 정형, 비정형 데이터에 대한 ETL 프로세스 설계 및 구현
- 다양한 데이터 수집 및 데이터 출처의 발굴
- 각종 job들의 자동 처리

자격요건
- Hadoop 데이터와 친숙하신 분을 모시고 싶습니다.
- RDBMS와 Hadoop을 자유롭게 오가며, 데이터 바다에서 Spark/Hive등으로 자유형 가능하신 분을 찾습니다.
- 각종 Parser개발과 Web Scraping에 익숙하시면 더욱 좋습니다.

# 데이터 엔지니어에게 필요한 기술

마지막으로 데이터 분야 채용 공고를 살펴보면, 어떤 기술을 사용하고 있고 공부해야 하는지 대략적으로 파악할 수 있습니다. 

아래는 K 사 채용공고 몇 개의 링크입니다. 내용을 확인해 보시면서 어떤 기술을 사용하고 있구나, 한번 파악해보시는 것도 좋습니다.

데이터 엔지니어(Data Engineer)가 하는 일 

큰 규모의 확장성 높은 시스템을 설계/구축한다. 
데이터 처리 시스템 성능 최적화 작업을 수행한다.

데이터를 질의 가능하게 만든다는 관점으로 보면, 랭킹 시스템/검색 시스템도 데이터 엔지니어링이라 볼 수 있음
맵리듀스 논문. 프로토콜 버퍼. 직렬화/역직렬화 논문을 읽어보길
하둡, 하둡을 쉽게 사용하기 위한 하이브, 하둡 2.0 생기고 스케쥴러 yarn이 별도로 나옴
지금은 batch 일을 하다보니 하둡 자체는 잡 스케쥴러 yarn과 자체 데이터센터인 hdfs 외에는 크게 커다란 비중을 차지하고 있진 않은 듯
하둡의 IO는 디스크에서 발생하고 스파크는 메모리! 스파크가 강세
denormalization, schemaless한 데이터를 다루고, nosql
 데이터 전처리 작업을 수행한다. 


데이터 엔지니어링 : 데이터를 질의 가능하게 처리. 데이터 과학자가 할 수 있을정도로 만들고, 데이터 엔지니어링이 그 밑단 데이터 준비
등등

• 게임에서 발생하는 데이터를 적재하고 조회할 수 있도록 데이터 인프라의 자동화
• Spark(혹은 그에 준하는 대용량 데이터 분산 처리 기술) 기반의 데이터 분산처리 및 가공해서 적재
• 데이터 인프라의 자동화
• 수집한 데이터 기반으로 다양한 알고리즘 및 방법론을 적용하여 어뷰징 행위 탐지 및 게임 라이브 환경에 피드백
주요업무
- 빅데이터 플랫폼 담당자
- 인프라 최적화 및 안정화
- 데이터 분석 로직 구현 및 개선
- 데이터 파이프 라인 유지 보수

우대사항
- 1가지 이상 언어에 자신이 있고 다른 언어에 대해서 두려움이 없는 분(java, scala, python)
- 빅데이터 플랫폼 실무 경력이 있는 분
- Spark / ELK / AWS 중 하나 이상 경험하신 분
- 설치형 서비스를 경험하신 분
